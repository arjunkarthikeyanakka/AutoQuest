{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b06be-8904-4431-9d41-60d3ecda987c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: wandb in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.19.9)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: setproctitle in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: rouge_score in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.1)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: hf_xet in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: logger in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "!pip install hf_xet\n",
    "!pip install logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3d9b1-8797-4531-a234-05b9098f2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertModel, \n",
    "    BertTokenizer, \n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    EncoderDecoderModel, \n",
    "    EncoderDecoderConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a025a2-db8c-4638-9438-cdc0238f3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c800b-e7cd-45c8-a3d9-0e040d2056de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d9995-c8d5-4bbc-ae2d-e534742db695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebbbdc6-11dc-40c8-86e2-e1cf3c7f05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerationDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Context-question pairs are prepared for encoder-decoder model training using the dataset.\n",
    "        Encodes the question as the GPT-2 target and the context with the BERT difficulty tag.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, bert_tokenizer, gpt2_tokenizer, max_encoder_length=512, max_decoder_length=64):\n",
    "        self.data = data\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.gpt2_tokenizer = gpt2_tokenizer\n",
    "        self.max_encoder_length = max_encoder_length\n",
    "        self.max_decoder_length = max_decoder_length\n",
    "        \n",
    "        special_tokens = {\"additional_special_tokens\": [\"[EASY]\", \"[MEDIUM]\", \"[HARD]\", \"[CONTEXT]\", \"[QUESTION]\"]}\n",
    "        if self.bert_tokenizer.add_special_tokens(special_tokens):\n",
    "            logger.info(\"Added special tokens to BERT tokenizer\")\n",
    "        \n",
    "        if self.gpt2_tokenizer.add_special_tokens(special_tokens):\n",
    "            logger.info(\"Added special tokens to GPT2 tokenizer\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item[\"context\"]\n",
    "        \n",
    "        qa_pair = random.choice(item[\"qa_pairs\"])\n",
    "        question = qa_pair[\"question\"]\n",
    "        difficulty = qa_pair[\"difficulty\"]\n",
    "        \n",
    "        difficulty_tag = f\"[{difficulty.upper()}]\"\n",
    "        formatted_input = f\"[CONTEXT] {context} {difficulty_tag} [QUESTION]\"\n",
    "        \n",
    "        encoder_inputs = self.bert_tokenizer(\n",
    "            formatted_input,\n",
    "            max_length=self.max_encoder_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        decoder_inputs = self.gpt2_tokenizer(\n",
    "            question,\n",
    "            max_length=self.max_decoder_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoder_inputs.input_ids.squeeze(),\n",
    "            \"attention_mask\": encoder_inputs.attention_mask.squeeze(),\n",
    "            \"decoder_input_ids\": decoder_inputs.input_ids.squeeze(),\n",
    "            \"decoder_attention_mask\": decoder_inputs.attention_mask.squeeze(),\n",
    "            \"labels\": decoder_inputs.input_ids.squeeze().clone(),\n",
    "            \"raw_context\": context,\n",
    "            \"raw_question\": question,\n",
    "            \"difficulty\": difficulty\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31c0c166-bddc-4523-80d8-dcbeacd78459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Load dataset from JSON file\"\"\"\n",
    "    if data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use a JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e286c18-7c33-4ae5-9da7-71dbe54d7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_gpt2_model():\n",
    "    \"\"\"Create and initialize the BERT-GPT2 encoder-decoder model with improved connection\"\"\"\n",
    "    bert_model_name = \"bert-base-uncased\"\n",
    "    gpt2_model_name = \"gpt2\"\n",
    "    \n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "    \n",
    "    if gpt2_tokenizer.pad_token is None:\n",
    "        gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    \n",
    "    model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        bert_model_name,\n",
    "        gpt2_model_name\n",
    "    )\n",
    "    \n",
    "    model.config.add_cross_attention = True\n",
    "    model.config.decoder.add_cross_attention = True\n",
    "    \n",
    "    model.config.decoder_start_token_id = gpt2_tokenizer.bos_token_id\n",
    "    model.config.eos_token_id = gpt2_tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = gpt2_tokenizer.pad_token_id\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "    \n",
    "    special_tokens = {\"additional_special_tokens\": [\"[EASY]\", \"[MEDIUM]\", \"[HARD]\", \"[CONTEXT]\", \"[QUESTION]\"]}\n",
    "    bert_tokenizer.add_special_tokens(special_tokens)\n",
    "    gpt2_tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    model.encoder.resize_token_embeddings(len(bert_tokenizer))\n",
    "    model.decoder.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "    \n",
    "    for name, param in model.encoder.named_parameters():\n",
    "        if 'layer.10' not in name and 'layer.11' not in name:\n",
    "            param.requires_grad = False\n",
    "            logger.info(f\"Freezing parameter: {name}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Total params: {total_params:,} | Trainable params: {trainable_params:,} | \" \n",
    "               f\"Frozen params: {total_params - trainable_params:,}\")\n",
    "    \n",
    "    return model, bert_tokenizer, gpt2_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21329d2b-a3fb-4bbb-8b9a-ab76b4c8ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data, bert_tokenizer, gpt2_tokenizer, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"Split the data into train, validation, and test sets\"\"\"\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Split ratios must sum to 1\"\n",
    "    \n",
    "    train_data, temp_data = train_test_split(data, test_size=(val_size + test_size), random_state=42)\n",
    "    \n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data, \n",
    "        test_size=test_size/(val_size + test_size), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Data split: Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    train_dataset = QuestionGenerationDataset(train_data, bert_tokenizer, gpt2_tokenizer)\n",
    "    val_dataset = QuestionGenerationDataset(val_data, bert_tokenizer, gpt2_tokenizer)\n",
    "    test_dataset = QuestionGenerationDataset(test_data, bert_tokenizer, gpt2_tokenizer)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be46242-1bcd-4e2f-a1ed-2512597f44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, num_epochs=3):\n",
    "    \"\"\"Train the model with improved training strategy\"\"\"\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "        \n",
    "        for batch in train_progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "                val_progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Average validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            logger.info(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            output_dir = \"bert_gpt2_qg_model\"\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            model_path = os.path.join(output_dir, \"best_model.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            logger.info(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8029ed02-7733-45c4-aba8-a22d3e41bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def calculate_bleu_score(references, candidates):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for generated questions.\n",
    "    \"\"\"\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    \n",
    "    for ref, cand in zip(references, candidates):\n",
    "        ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "        cand_tokens = nltk.word_tokenize(cand.lower())\n",
    "        \n",
    "        score = sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68f9dc7d-eaca-4e90-a762-76a2e3884f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader, gpt2_tokenizer, device):\n",
    "    \"\"\"Evaluate the model and calculate BLEU score\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Generating questions\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=64,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "                length_penalty=1.0,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "            \n",
    "            predictions = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = batch[\"raw_question\"]\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_references.extend(references)\n",
    "    \n",
    "    bleu_score = calculate_bleu_score(all_references, all_predictions)\n",
    "    \n",
    "    logger.info(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "    return bleu_score, all_predictions, all_references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3cc1d38-5074-4f6c-9a52-abc7fe51dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(model, bert_tokenizer, gpt2_tokenizer, context, difficulty='medium', num_questions=3, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate diverse questions for a given context using improved generation strategies\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    question_types = {\n",
    "        \"easy\": [\n",
    "            \"factual\", \"definition\", \"identification\", \"basic concept\", \"simple explanation\"\n",
    "        ],\n",
    "        \"medium\": [\n",
    "            \"analytical\", \"inference\", \"connection\", \"application\", \"comparison\"\n",
    "        ],\n",
    "        \"hard\": [\n",
    "            \"evaluation\", \"synthesis\", \"critique\", \"hypothetical\", \"implications\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    generated_questions = []\n",
    "    unique_questions = set()\n",
    "    \n",
    "    for i in range(min(len(question_types[difficulty]), num_questions * 2)): \n",
    "        q_type = question_types[difficulty][i % len(question_types[difficulty])]\n",
    "        \n",
    "        formatted_input = f\"[CONTEXT] {context} [{difficulty.upper()}] Generate a {q_type} question. [QUESTION]\"\n",
    "        \n",
    "        encoder_inputs = bert_tokenizer(\n",
    "            formatted_input,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=encoder_inputs.input_ids,\n",
    "                attention_mask=encoder_inputs.attention_mask,\n",
    "                max_length=48 + (i * 2), \n",
    "                do_sample=True,\n",
    "                top_p=0.92 - (i * 0.02),  \n",
    "                temperature=0.7 + (i * 0.05),\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "                length_penalty=1.0,\n",
    "                repetition_penalty=1.2 + (i * 0.1),\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        question = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        question = clean_question(question)\n",
    "        \n",
    "        if question and question not in unique_questions:\n",
    "            unique_questions.add(question)\n",
    "            generated_questions.append(question)\n",
    "    \n",
    "    filtered_questions = filter_questions(generated_questions, context)\n",
    "    \n",
    "    question_templates = {\n",
    "        \"easy\": [\n",
    "            \"What is {topic}?\",\n",
    "            \"Can you define {topic}?\",\n",
    "            \"What does {topic} refer to?\"\n",
    "        ],\n",
    "        \"medium\": [\n",
    "            \"How does {topic} impact {related_topic}?\",\n",
    "            \"What connection exists between {topic} and {related_topic}?\",\n",
    "            \"Why is {topic} important to understand?\"\n",
    "        ],\n",
    "        \"hard\": [\n",
    "            \"What would happen if {topic} was fundamentally different?\",\n",
    "            \"How might future developments in {topic} affect {related_topic}?\",\n",
    "            \"What are the broader implications of {topic} for society?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    topics = extract_topics(context)\n",
    "    \n",
    "    while len(filtered_questions) < num_questions and topics:\n",
    "        template = random.choice(question_templates[difficulty])\n",
    "        topic = topics.pop(0) if topics else \"this topic\"\n",
    "        related_topic = topics.pop(0) if topics else \"related areas\"\n",
    "        \n",
    "        templated_question = template.format(topic=topic, related_topic=related_topic)\n",
    "        if templated_question not in unique_questions:\n",
    "            unique_questions.add(templated_question)\n",
    "            filtered_questions.append(templated_question)\n",
    "    \n",
    "    return filtered_questions[:num_questions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c16d98-3ef3-4a9f-82a4-63c3f8970093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    \"\"\"Clean up a generated question\"\"\"\n",
    "    if \"?\" in question:\n",
    "        question = question.split(\"?\")[0] + \"?\"\n",
    "    \n",
    "    artifacts = [\"References\", \"Reply\", \"Delete\", \"Click\", \"Comments\", \"Contents\", \n",
    "                 \"Editors\", \"Source\", \"credit\", \"Please see\", \"More\", \"Thank\", \n",
    "                 \"credited\", \"Logged\", \"context\", \"*\"]\n",
    "                \n",
    "    for artifact in artifacts:\n",
    "        if artifact in question:\n",
    "            question = question.split(artifact)[0].strip()\n",
    "    \n",
    "    if not question.endswith(\"?\"):\n",
    "        question += \"?\"\n",
    "    \n",
    "    common_prefixes = [\"I think\", \"It seems\", \"Maybe\", \"Perhaps\", \"I believe\", \"Can you explain\"]\n",
    "    for prefix in common_prefixes:\n",
    "        if question.startswith(prefix):\n",
    "            rest_of_question = question[len(prefix):].strip()\n",
    "            if any(q_word in rest_of_question.lower() for q_word in [\"what\", \"why\", \"how\", \"when\", \"where\", \"which\", \"who\", \"can\", \"does\", \"is\", \"are\"]):\n",
    "                question = rest_of_question\n",
    "                \n",
    "    return question.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a10b2ee-7f01-411c-a947-e00fed5b9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_questions(questions, context):\n",
    "    \"\"\"Filter out low-quality questions\"\"\"\n",
    "    filtered = []\n",
    "    context_lower = context.lower()\n",
    "    \n",
    "    topics = extract_topics(context)\n",
    "    topic_words = set(word.lower() for word in topics)\n",
    "    \n",
    "    for question in questions:\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        is_relevant = any(topic in question_lower for topic in topic_words)\n",
    "        \n",
    "        has_question_word = any(q_word in question_lower for q_word in \n",
    "                             [\"what\", \"why\", \"how\", \"when\", \"where\", \"which\", \"who\",\n",
    "                              \"can\", \"does\", \"is\", \"are\"])\n",
    "        \n",
    "        proper_length = 15 <= len(question) <= 150\n",
    "        \n",
    "        if is_relevant and has_question_word and proper_length:\n",
    "            filtered.append(question)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61b4edb7-f3e3-4c9d-924d-19143f5b7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(context):\n",
    "    \"\"\"Extract key topics from context using basic NLP techniques\"\"\"\n",
    "    tokens = context.lower().split()\n",
    "    \n",
    "    stopwords = [\"the\", \"and\", \"is\", \"in\", \"it\", \"to\", \"that\", \"of\", \"for\", \"a\", \"an\", \"with\", \"by\"]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    token_counts = {}\n",
    "    for token in tokens:\n",
    "        if len(token) > 3:  \n",
    "            token_counts[token] = token_counts.get(token, 0) + 1\n",
    "    \n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [token for token, count in sorted_tokens[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de35f9-2267-473c-95fd-32bf2a0fb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge_scores(references, predictions):\n",
    "    \"\"\"Calculate Rouge scores between predicted and reference questions\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for ref, pred in zip(references, predictions):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores) if rouge1_scores else 0\n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores) if rouge2_scores else 0\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": avg_rouge1,\n",
    "        \"rouge2\": avg_rouge2,\n",
    "        \"rougeL\": avg_rougeL\n",
    "    }\n",
    "\n",
    "def evaluate_with_rouge(model, test_dataloader, gpt2_tokenizer, device):\n",
    "    \"\"\"Evaluate model and calculate Rouge scores\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    all_difficulties = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_dataloader):\n",
    "            logger.info(f\"Processing batch {i+1}/{len(test_dataloader)}\")\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            try:\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=150,\n",
    "                    min_length=5,\n",
    "                    num_beams=4,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                \n",
    "                predictions = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                references = batch[\"raw_question\"]\n",
    "                difficulties = batch[\"difficulty\"]\n",
    "                \n",
    "                all_predictions.extend(predictions)\n",
    "                all_references.extend(references)\n",
    "                all_difficulties.extend(difficulties)\n",
    "                \n",
    "                if i < 2:\n",
    "                    logger.info(\"\\nBatch examples:\")\n",
    "                    for j in range(min(2, len(predictions))):\n",
    "                        logger.info(f\"Difficulty: {difficulties[j]}\")\n",
    "                        logger.info(f\"Reference: {references[j]}\")\n",
    "                        logger.info(f\"Prediction: {predictions[j]}\")\n",
    "                        logger.info(\"-\" * 30)\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                logger.error(f\"Error in batch {i+1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not all_predictions:\n",
    "        logger.error(\"No predictions were generated. Check the model and generation parameters.\")\n",
    "        return {}, [], []\n",
    "    \n",
    "    rouge_scores = calculate_rouge_scores(all_references, all_predictions)\n",
    "    \n",
    "    for metric, score in rouge_scores.items():\n",
    "        logger.info(f\"{metric.upper()}: {score:.4f}\")\n",
    "    \n",
    "    difficulties = ['easy', 'medium', 'hard']\n",
    "    for difficulty in difficulties:\n",
    "        difficulty_indices = [i for i, d in enumerate(all_difficulties) if d == difficulty]\n",
    "        if not difficulty_indices:\n",
    "            continue\n",
    "            \n",
    "        diff_references = [all_references[i] for i in difficulty_indices]\n",
    "        diff_predictions = [all_predictions[i] for i in difficulty_indices]\n",
    "        \n",
    "        diff_rouge_scores = calculate_rouge_scores(diff_references, diff_predictions)\n",
    "        logger.info(f\"\\n{difficulty.upper()} questions:\")\n",
    "        for metric, score in diff_rouge_scores.items():\n",
    "            logger.info(f\"{metric.upper()}: {score:.4f}\")\n",
    "    \n",
    "    num_examples = min(5, len(all_predictions))\n",
    "    logger.info(\"\\nFinal examples:\")\n",
    "    for i in range(num_examples):\n",
    "        logger.info(f\"Difficulty: {all_difficulties[i]}\")\n",
    "        logger.info(f\"Reference: {all_references[i]}\")\n",
    "        logger.info(f\"Prediction: {all_predictions[i]}\")\n",
    "        logger.info(\"-\" * 40)\n",
    "    \n",
    "    return rouge_scores, all_predictions, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c9f12ba-64c0-400b-8b2b-a7548c67e743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 04:12:52,660 - __main__ - INFO - Using device: cuda\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-04-25 04:12:54,424 - __main__ - INFO - Freezing parameter: embeddings.word_embeddings.weight\n",
      "2025-04-25 04:12:54,425 - __main__ - INFO - Freezing parameter: embeddings.position_embeddings.weight\n",
      "2025-04-25 04:12:54,425 - __main__ - INFO - Freezing parameter: embeddings.token_type_embeddings.weight\n",
      "2025-04-25 04:12:54,426 - __main__ - INFO - Freezing parameter: embeddings.LayerNorm.weight\n",
      "2025-04-25 04:12:54,426 - __main__ - INFO - Freezing parameter: embeddings.LayerNorm.bias\n",
      "2025-04-25 04:12:54,427 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.self.query.weight\n",
      "2025-04-25 04:12:54,427 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.self.query.bias\n",
      "2025-04-25 04:12:54,427 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.self.key.weight\n",
      "2025-04-25 04:12:54,428 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.self.key.bias\n",
      "2025-04-25 04:12:54,428 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.self.value.weight\n",
      "2025-04-25 04:12:54,429 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.self.value.bias\n",
      "2025-04-25 04:12:54,429 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,429 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,430 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,430 - __main__ - INFO - Freezing parameter: encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,430 - __main__ - INFO - Freezing parameter: encoder.layer.0.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,431 - __main__ - INFO - Freezing parameter: encoder.layer.0.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,431 - __main__ - INFO - Freezing parameter: encoder.layer.0.output.dense.weight\n",
      "2025-04-25 04:12:54,431 - __main__ - INFO - Freezing parameter: encoder.layer.0.output.dense.bias\n",
      "2025-04-25 04:12:54,432 - __main__ - INFO - Freezing parameter: encoder.layer.0.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,432 - __main__ - INFO - Freezing parameter: encoder.layer.0.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,432 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.self.query.weight\n",
      "2025-04-25 04:12:54,433 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.self.query.bias\n",
      "2025-04-25 04:12:54,433 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.self.key.weight\n",
      "2025-04-25 04:12:54,433 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.self.key.bias\n",
      "2025-04-25 04:12:54,434 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.self.value.weight\n",
      "2025-04-25 04:12:54,434 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.self.value.bias\n",
      "2025-04-25 04:12:54,434 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,435 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,435 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,435 - __main__ - INFO - Freezing parameter: encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,436 - __main__ - INFO - Freezing parameter: encoder.layer.1.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,436 - __main__ - INFO - Freezing parameter: encoder.layer.1.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,436 - __main__ - INFO - Freezing parameter: encoder.layer.1.output.dense.weight\n",
      "2025-04-25 04:12:54,437 - __main__ - INFO - Freezing parameter: encoder.layer.1.output.dense.bias\n",
      "2025-04-25 04:12:54,437 - __main__ - INFO - Freezing parameter: encoder.layer.1.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,438 - __main__ - INFO - Freezing parameter: encoder.layer.1.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,438 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.self.query.weight\n",
      "2025-04-25 04:12:54,438 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.self.query.bias\n",
      "2025-04-25 04:12:54,439 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.self.key.weight\n",
      "2025-04-25 04:12:54,439 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.self.key.bias\n",
      "2025-04-25 04:12:54,439 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.self.value.weight\n",
      "2025-04-25 04:12:54,440 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.self.value.bias\n",
      "2025-04-25 04:12:54,440 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,440 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,441 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,441 - __main__ - INFO - Freezing parameter: encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,442 - __main__ - INFO - Freezing parameter: encoder.layer.2.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,442 - __main__ - INFO - Freezing parameter: encoder.layer.2.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,442 - __main__ - INFO - Freezing parameter: encoder.layer.2.output.dense.weight\n",
      "2025-04-25 04:12:54,443 - __main__ - INFO - Freezing parameter: encoder.layer.2.output.dense.bias\n",
      "2025-04-25 04:12:54,443 - __main__ - INFO - Freezing parameter: encoder.layer.2.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,443 - __main__ - INFO - Freezing parameter: encoder.layer.2.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,444 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.self.query.weight\n",
      "2025-04-25 04:12:54,444 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.self.query.bias\n",
      "2025-04-25 04:12:54,444 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.self.key.weight\n",
      "2025-04-25 04:12:54,445 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.self.key.bias\n",
      "2025-04-25 04:12:54,445 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.self.value.weight\n",
      "2025-04-25 04:12:54,445 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.self.value.bias\n",
      "2025-04-25 04:12:54,446 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,446 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,446 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,447 - __main__ - INFO - Freezing parameter: encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,447 - __main__ - INFO - Freezing parameter: encoder.layer.3.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,448 - __main__ - INFO - Freezing parameter: encoder.layer.3.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,448 - __main__ - INFO - Freezing parameter: encoder.layer.3.output.dense.weight\n",
      "2025-04-25 04:12:54,448 - __main__ - INFO - Freezing parameter: encoder.layer.3.output.dense.bias\n",
      "2025-04-25 04:12:54,448 - __main__ - INFO - Freezing parameter: encoder.layer.3.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,449 - __main__ - INFO - Freezing parameter: encoder.layer.3.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,449 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.self.query.weight\n",
      "2025-04-25 04:12:54,450 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.self.query.bias\n",
      "2025-04-25 04:12:54,450 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.self.key.weight\n",
      "2025-04-25 04:12:54,450 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.self.key.bias\n",
      "2025-04-25 04:12:54,451 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.self.value.weight\n",
      "2025-04-25 04:12:54,451 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.self.value.bias\n",
      "2025-04-25 04:12:54,451 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,452 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,452 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,453 - __main__ - INFO - Freezing parameter: encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,453 - __main__ - INFO - Freezing parameter: encoder.layer.4.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,454 - __main__ - INFO - Freezing parameter: encoder.layer.4.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,465 - __main__ - INFO - Freezing parameter: encoder.layer.4.output.dense.weight\n",
      "2025-04-25 04:12:54,465 - __main__ - INFO - Freezing parameter: encoder.layer.4.output.dense.bias\n",
      "2025-04-25 04:12:54,466 - __main__ - INFO - Freezing parameter: encoder.layer.4.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,466 - __main__ - INFO - Freezing parameter: encoder.layer.4.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,467 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.self.query.weight\n",
      "2025-04-25 04:12:54,467 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.self.query.bias\n",
      "2025-04-25 04:12:54,467 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.self.key.weight\n",
      "2025-04-25 04:12:54,468 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.self.key.bias\n",
      "2025-04-25 04:12:54,468 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.self.value.weight\n",
      "2025-04-25 04:12:54,468 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.self.value.bias\n",
      "2025-04-25 04:12:54,469 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,469 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,469 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,470 - __main__ - INFO - Freezing parameter: encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,470 - __main__ - INFO - Freezing parameter: encoder.layer.5.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,470 - __main__ - INFO - Freezing parameter: encoder.layer.5.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,471 - __main__ - INFO - Freezing parameter: encoder.layer.5.output.dense.weight\n",
      "2025-04-25 04:12:54,471 - __main__ - INFO - Freezing parameter: encoder.layer.5.output.dense.bias\n",
      "2025-04-25 04:12:54,471 - __main__ - INFO - Freezing parameter: encoder.layer.5.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,472 - __main__ - INFO - Freezing parameter: encoder.layer.5.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,472 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.self.query.weight\n",
      "2025-04-25 04:12:54,473 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.self.query.bias\n",
      "2025-04-25 04:12:54,473 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.self.key.weight\n",
      "2025-04-25 04:12:54,474 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.self.key.bias\n",
      "2025-04-25 04:12:54,474 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.self.value.weight\n",
      "2025-04-25 04:12:54,474 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.self.value.bias\n",
      "2025-04-25 04:12:54,475 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,475 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,476 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,476 - __main__ - INFO - Freezing parameter: encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,476 - __main__ - INFO - Freezing parameter: encoder.layer.6.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,477 - __main__ - INFO - Freezing parameter: encoder.layer.6.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,477 - __main__ - INFO - Freezing parameter: encoder.layer.6.output.dense.weight\n",
      "2025-04-25 04:12:54,478 - __main__ - INFO - Freezing parameter: encoder.layer.6.output.dense.bias\n",
      "2025-04-25 04:12:54,478 - __main__ - INFO - Freezing parameter: encoder.layer.6.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,478 - __main__ - INFO - Freezing parameter: encoder.layer.6.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,479 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.self.query.weight\n",
      "2025-04-25 04:12:54,479 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.self.query.bias\n",
      "2025-04-25 04:12:54,479 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.self.key.weight\n",
      "2025-04-25 04:12:54,480 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.self.key.bias\n",
      "2025-04-25 04:12:54,480 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.self.value.weight\n",
      "2025-04-25 04:12:54,481 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.self.value.bias\n",
      "2025-04-25 04:12:54,481 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,481 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,482 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,482 - __main__ - INFO - Freezing parameter: encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,482 - __main__ - INFO - Freezing parameter: encoder.layer.7.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,483 - __main__ - INFO - Freezing parameter: encoder.layer.7.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,483 - __main__ - INFO - Freezing parameter: encoder.layer.7.output.dense.weight\n",
      "2025-04-25 04:12:54,484 - __main__ - INFO - Freezing parameter: encoder.layer.7.output.dense.bias\n",
      "2025-04-25 04:12:54,484 - __main__ - INFO - Freezing parameter: encoder.layer.7.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,484 - __main__ - INFO - Freezing parameter: encoder.layer.7.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,485 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.self.query.weight\n",
      "2025-04-25 04:12:54,485 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.self.query.bias\n",
      "2025-04-25 04:12:54,485 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.self.key.weight\n",
      "2025-04-25 04:12:54,486 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.self.key.bias\n",
      "2025-04-25 04:12:54,486 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.self.value.weight\n",
      "2025-04-25 04:12:54,486 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.self.value.bias\n",
      "2025-04-25 04:12:54,487 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,487 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,492 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,492 - __main__ - INFO - Freezing parameter: encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,493 - __main__ - INFO - Freezing parameter: encoder.layer.8.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,493 - __main__ - INFO - Freezing parameter: encoder.layer.8.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,494 - __main__ - INFO - Freezing parameter: encoder.layer.8.output.dense.weight\n",
      "2025-04-25 04:12:54,494 - __main__ - INFO - Freezing parameter: encoder.layer.8.output.dense.bias\n",
      "2025-04-25 04:12:54,494 - __main__ - INFO - Freezing parameter: encoder.layer.8.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,495 - __main__ - INFO - Freezing parameter: encoder.layer.8.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,495 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.self.query.weight\n",
      "2025-04-25 04:12:54,495 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.self.query.bias\n",
      "2025-04-25 04:12:54,496 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.self.key.weight\n",
      "2025-04-25 04:12:54,496 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.self.key.bias\n",
      "2025-04-25 04:12:54,496 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.self.value.weight\n",
      "2025-04-25 04:12:54,497 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.self.value.bias\n",
      "2025-04-25 04:12:54,497 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.output.dense.weight\n",
      "2025-04-25 04:12:54,498 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.output.dense.bias\n",
      "2025-04-25 04:12:54,498 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,498 - __main__ - INFO - Freezing parameter: encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,499 - __main__ - INFO - Freezing parameter: encoder.layer.9.intermediate.dense.weight\n",
      "2025-04-25 04:12:54,499 - __main__ - INFO - Freezing parameter: encoder.layer.9.intermediate.dense.bias\n",
      "2025-04-25 04:12:54,499 - __main__ - INFO - Freezing parameter: encoder.layer.9.output.dense.weight\n",
      "2025-04-25 04:12:54,500 - __main__ - INFO - Freezing parameter: encoder.layer.9.output.dense.bias\n",
      "2025-04-25 04:12:54,500 - __main__ - INFO - Freezing parameter: encoder.layer.9.output.LayerNorm.weight\n",
      "2025-04-25 04:12:54,501 - __main__ - INFO - Freezing parameter: encoder.layer.9.output.LayerNorm.bias\n",
      "2025-04-25 04:12:54,501 - __main__ - INFO - Freezing parameter: pooler.dense.weight\n",
      "2025-04-25 04:12:54,501 - __main__ - INFO - Freezing parameter: pooler.dense.bias\n",
      "2025-04-25 04:12:54,504 - __main__ - INFO - Total params: 262,296,576 | Trainable params: 166,986,240 | Frozen params: 95,310,336\n",
      "2025-04-25 04:12:54,840 - __main__ - INFO - Data split: Train: 5061, Validation: 633, Test: 633\n",
      "Epoch 1/1 [Training]: 100%|██████████| 1266/1266 [07:06<00:00,  2.97it/s, loss=0.254]\n",
      "2025-04-25 04:20:01,334 - __main__ - INFO - Epoch 1/1 - Average training loss: 0.8561\n",
      "Epoch 1/1 [Validation]: 100%|██████████| 159/159 [00:24<00:00,  6.57it/s, loss=0.163]\n",
      "2025-04-25 04:20:25,528 - __main__ - INFO - Epoch 1/1 - Average validation loss: 0.5092\n",
      "2025-04-25 04:20:25,529 - __main__ - INFO - New best validation loss: 0.5092\n",
      "2025-04-25 04:20:29,929 - __main__ - INFO - Model saved to bert_gpt2_qg_model/best_model.pt\n",
      "2025-04-25 04:20:30,487 - __main__ - INFO - Loaded best model from bert_gpt2_qg_model/best_model.pt\n",
      "2025-04-25 04:20:30,488 - __main__ - INFO - \n",
      "--- BLEU Score Evaluation ---\n",
      "Generating questions:   0%|          | 0/159 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   1%|          | 1/159 [00:01<02:49,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   1%|▏         | 2/159 [00:02<02:48,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   2%|▏         | 3/159 [00:03<02:46,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   3%|▎         | 4/159 [00:04<02:42,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   3%|▎         | 5/159 [00:05<02:46,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   4%|▍         | 6/159 [00:06<02:58,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   4%|▍         | 7/159 [00:08<03:10,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   5%|▌         | 8/159 [00:09<03:03,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   6%|▌         | 9/159 [00:11<03:31,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   6%|▋         | 10/159 [00:12<03:11,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   7%|▋         | 11/159 [00:13<03:03,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   8%|▊         | 12/159 [00:14<02:54,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   8%|▊         | 13/159 [00:15<02:57,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   9%|▉         | 14/159 [00:16<02:52,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:   9%|▉         | 15/159 [00:17<02:46,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  10%|█         | 16/159 [00:18<02:41,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  11%|█         | 17/159 [00:20<02:40,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  11%|█▏        | 18/159 [00:21<02:36,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  12%|█▏        | 19/159 [00:22<02:34,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  13%|█▎        | 20/159 [00:23<02:49,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  13%|█▎        | 21/159 [00:24<02:50,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  14%|█▍        | 22/159 [00:26<02:48,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  14%|█▍        | 23/159 [00:27<02:40,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  15%|█▌        | 24/159 [00:28<02:37,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 25/159 [00:29<02:30,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  16%|█▋        | 26/159 [00:30<02:37,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  17%|█▋        | 27/159 [00:32<02:48,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  18%|█▊        | 28/159 [00:33<02:49,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  18%|█▊        | 29/159 [00:34<02:42,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 30/159 [00:35<02:34,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 31/159 [00:36<02:31,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  20%|██        | 32/159 [00:38<02:33,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  21%|██        | 33/159 [00:39<02:32,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  21%|██▏       | 34/159 [00:40<02:28,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 35/159 [00:41<02:33,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  23%|██▎       | 36/159 [00:43<02:31,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  23%|██▎       | 37/159 [00:44<02:24,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  24%|██▍       | 38/159 [00:45<02:21,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  25%|██▍       | 39/159 [00:46<02:19,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 40/159 [00:47<02:22,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  26%|██▌       | 41/159 [00:48<02:19,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  26%|██▋       | 42/159 [00:50<02:23,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  27%|██▋       | 43/159 [00:51<02:16,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 44/159 [00:52<02:04,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 45/159 [00:53<02:14,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  29%|██▉       | 46/159 [00:54<02:12,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  30%|██▉       | 47/159 [00:55<02:07,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  30%|███       | 48/159 [00:56<02:02,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  31%|███       | 49/159 [00:58<02:11,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 50/159 [00:59<02:08,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  32%|███▏      | 51/159 [01:00<02:10,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  33%|███▎      | 52/159 [01:01<02:05,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  33%|███▎      | 53/159 [01:02<02:05,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 54/159 [01:03<02:00,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  35%|███▍      | 55/159 [01:05<01:59,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  35%|███▌      | 56/159 [01:06<01:53,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  36%|███▌      | 57/159 [01:07<01:53,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  36%|███▋      | 58/159 [01:08<01:51,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  37%|███▋      | 59/159 [01:09<02:01,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 60/159 [01:11<02:02,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 61/159 [01:12<01:56,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  39%|███▉      | 62/159 [01:13<01:57,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  40%|███▉      | 63/159 [01:14<02:02,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  40%|████      | 64/159 [01:16<02:01,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  41%|████      | 65/159 [01:17<01:58,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  42%|████▏     | 66/159 [01:18<01:55,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  42%|████▏     | 67/159 [01:19<01:55,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  43%|████▎     | 68/159 [01:20<01:49,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  43%|████▎     | 69/159 [01:22<01:48,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 70/159 [01:23<01:43,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  45%|████▍     | 71/159 [01:24<01:41,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  45%|████▌     | 72/159 [01:25<01:38,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  46%|████▌     | 73/159 [01:26<01:42,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 74/159 [01:27<01:40,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 75/159 [01:29<01:50,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  48%|████▊     | 76/159 [01:30<01:49,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  48%|████▊     | 77/159 [01:32<01:45,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  49%|████▉     | 78/159 [01:33<01:39,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  50%|████▉     | 79/159 [01:34<01:44,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  50%|█████     | 80/159 [01:35<01:35,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  51%|█████     | 81/159 [01:37<01:49,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  52%|█████▏    | 82/159 [01:38<01:40,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  52%|█████▏    | 83/159 [01:39<01:32,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 84/159 [01:40<01:26,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 85/159 [01:41<01:24,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  54%|█████▍    | 86/159 [01:42<01:23,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  55%|█████▍    | 87/159 [01:43<01:22,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  55%|█████▌    | 88/159 [01:45<01:24,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  56%|█████▌    | 89/159 [01:46<01:22,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  57%|█████▋    | 90/159 [01:47<01:20,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  57%|█████▋    | 91/159 [01:48<01:21,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  58%|█████▊    | 92/159 [01:50<01:26,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  58%|█████▊    | 93/159 [01:51<01:23,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 94/159 [01:52<01:21,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  60%|█████▉    | 95/159 [01:53<01:17,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  60%|██████    | 96/159 [01:55<01:18,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  61%|██████    | 97/159 [01:56<01:15,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  62%|██████▏   | 98/159 [01:57<01:13,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  62%|██████▏   | 99/159 [01:58<01:10,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  63%|██████▎   | 100/159 [01:59<01:11,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  64%|██████▎   | 101/159 [02:00<01:07,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  64%|██████▍   | 102/159 [02:02<01:10,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  65%|██████▍   | 103/159 [02:03<01:07,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  65%|██████▌   | 104/159 [02:04<01:03,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 105/159 [02:05<01:01,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  67%|██████▋   | 106/159 [02:07<01:08,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  67%|██████▋   | 107/159 [02:08<01:05,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  68%|██████▊   | 108/159 [02:09<01:02,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  69%|██████▊   | 109/159 [02:10<01:01,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 110/159 [02:11<00:58,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  70%|██████▉   | 111/159 [02:13<00:56,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  70%|███████   | 112/159 [02:14<00:54,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  71%|███████   | 113/159 [02:15<00:52,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 114/159 [02:16<00:49,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 115/159 [02:17<00:50,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  73%|███████▎  | 116/159 [02:18<00:50,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  74%|███████▎  | 117/159 [02:20<00:52,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  74%|███████▍  | 118/159 [02:21<00:48,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  75%|███████▍  | 119/159 [02:22<00:46,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 120/159 [02:23<00:43,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  76%|███████▌  | 121/159 [02:24<00:42,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  77%|███████▋  | 122/159 [02:25<00:42,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  77%|███████▋  | 123/159 [02:26<00:40,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 124/159 [02:27<00:39,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  79%|███████▊  | 125/159 [02:29<00:40,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  79%|███████▉  | 126/159 [02:30<00:38,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  80%|███████▉  | 127/159 [02:31<00:37,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  81%|████████  | 128/159 [02:32<00:36,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  81%|████████  | 129/159 [02:33<00:34,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  82%|████████▏ | 130/159 [02:34<00:32,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  82%|████████▏ | 131/159 [02:36<00:34,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  83%|████████▎ | 132/159 [02:37<00:31,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  84%|████████▎ | 133/159 [02:38<00:30,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 134/159 [02:39<00:27,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  85%|████████▍ | 135/159 [02:40<00:26,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  86%|████████▌ | 136/159 [02:41<00:25,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  86%|████████▌ | 137/159 [02:43<00:26,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  87%|████████▋ | 138/159 [02:44<00:25,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  87%|████████▋ | 139/159 [02:45<00:24,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 140/159 [02:47<00:24,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  89%|████████▊ | 141/159 [02:48<00:23,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  89%|████████▉ | 142/159 [02:49<00:20,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  90%|████████▉ | 143/159 [02:50<00:18,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 144/159 [02:51<00:18,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 145/159 [02:53<00:17,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  92%|█████████▏| 146/159 [02:54<00:16,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  92%|█████████▏| 147/159 [02:55<00:15,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  93%|█████████▎| 148/159 [02:56<00:13,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  94%|█████████▎| 149/159 [02:57<00:11,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 150/159 [02:59<00:11,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  95%|█████████▍| 151/159 [03:00<00:09,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  96%|█████████▌| 152/159 [03:01<00:08,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  96%|█████████▌| 153/159 [03:02<00:07,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 154/159 [03:04<00:06,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 155/159 [03:05<00:04,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  98%|█████████▊| 156/159 [03:06<00:03,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  99%|█████████▊| 157/159 [03:07<00:02,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions:  99%|█████████▉| 158/159 [03:09<00:01,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating questions: 100%|██████████| 159/159 [03:09<00:00,  1.19s/it]\n",
      "2025-04-25 04:23:40,011 - __main__ - INFO - BLEU Score: 0.0484\n",
      "2025-04-25 04:23:40,012 - __main__ - INFO - \n",
      "--- ROUGE Score Evaluation ---\n",
      "2025-04-25 04:23:40,041 - __main__ - INFO - Processing batch 1/5\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:40,588 - __main__ - INFO - \n",
      "Batch examples:\n",
      "2025-04-25 04:23:40,589 - __main__ - INFO - Difficulty: medium\n",
      "2025-04-25 04:23:40,590 - __main__ - INFO - Reference: What is the goal of this response?\n",
      "2025-04-25 04:23:40,590 - __main__ - INFO - Prediction: What is the purpose of this paragraph?\n",
      "2025-04-25 04:23:40,590 - __main__ - INFO - ------------------------------\n",
      "2025-04-25 04:23:40,591 - __main__ - INFO - Difficulty: easy\n",
      "2025-04-25 04:23:40,591 - __main__ - INFO - Reference: When was the Higgs boson discovered?\n",
      "2025-04-25 04:23:40,592 - __main__ - INFO - Prediction: What is the main purpose of quantum mechanics?\n",
      "2025-04-25 04:23:40,592 - __main__ - INFO - ------------------------------\n",
      "2025-04-25 04:23:40,598 - __main__ - INFO - Processing batch 2/5\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:41,074 - __main__ - INFO - \n",
      "Batch examples:\n",
      "2025-04-25 04:23:41,074 - __main__ - INFO - Difficulty: hard\n",
      "2025-04-25 04:23:41,075 - __main__ - INFO - Reference: What is the sole permissible format for the response?\n",
      "2025-04-25 04:23:41,075 - __main__ - INFO - Prediction: What is the purpose of this paragraph?\n",
      "2025-04-25 04:23:41,076 - __main__ - INFO - ------------------------------\n",
      "2025-04-25 04:23:41,076 - __main__ - INFO - Difficulty: hard\n",
      "2025-04-25 04:23:41,076 - __main__ - INFO - Reference: Why would the identification of dark matter be considered a major breakthrough?\n",
      "2025-04-25 04:23:41,077 - __main__ - INFO - Prediction: What is the main goal of quantum mechanics?\n",
      "2025-04-25 04:23:41,077 - __main__ - INFO - ------------------------------\n",
      "2025-04-25 04:23:41,083 - __main__ - INFO - Processing batch 3/5\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:41,557 - __main__ - INFO - Processing batch 4/5\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:42,116 - __main__ - INFO - Processing batch 5/5\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:42,593 - absl - INFO - Using default tokenizer.\n",
      "2025-04-25 04:23:42,597 - __main__ - INFO - ROUGE1: 0.3241\n",
      "2025-04-25 04:23:42,597 - __main__ - INFO - ROUGE2: 0.1119\n",
      "2025-04-25 04:23:42,598 - __main__ - INFO - ROUGEL: 0.3241\n",
      "2025-04-25 04:23:42,598 - absl - INFO - Using default tokenizer.\n",
      "2025-04-25 04:23:42,600 - __main__ - INFO - \n",
      "EASY questions:\n",
      "2025-04-25 04:23:42,600 - __main__ - INFO - ROUGE1: 0.3766\n",
      "2025-04-25 04:23:42,601 - __main__ - INFO - ROUGE2: 0.1111\n",
      "2025-04-25 04:23:42,602 - __main__ - INFO - ROUGEL: 0.3766\n",
      "2025-04-25 04:23:42,602 - absl - INFO - Using default tokenizer.\n",
      "2025-04-25 04:23:42,604 - __main__ - INFO - \n",
      "MEDIUM questions:\n",
      "2025-04-25 04:23:42,604 - __main__ - INFO - ROUGE1: 0.3919\n",
      "2025-04-25 04:23:42,605 - __main__ - INFO - ROUGE2: 0.1667\n",
      "2025-04-25 04:23:42,605 - __main__ - INFO - ROUGEL: 0.3919\n",
      "2025-04-25 04:23:42,606 - absl - INFO - Using default tokenizer.\n",
      "2025-04-25 04:23:42,608 - __main__ - INFO - \n",
      "HARD questions:\n",
      "2025-04-25 04:23:42,608 - __main__ - INFO - ROUGE1: 0.2338\n",
      "2025-04-25 04:23:42,609 - __main__ - INFO - ROUGE2: 0.0714\n",
      "2025-04-25 04:23:42,609 - __main__ - INFO - ROUGEL: 0.2338\n",
      "2025-04-25 04:23:42,610 - __main__ - INFO - \n",
      "Final examples:\n",
      "2025-04-25 04:23:42,610 - __main__ - INFO - Difficulty: medium\n",
      "2025-04-25 04:23:42,611 - __main__ - INFO - Reference: What is the goal of this response?\n",
      "2025-04-25 04:23:42,612 - __main__ - INFO - Prediction: What is the purpose of this paragraph?\n",
      "2025-04-25 04:23:42,612 - __main__ - INFO - ----------------------------------------\n",
      "2025-04-25 04:23:42,613 - __main__ - INFO - Difficulty: easy\n",
      "2025-04-25 04:23:42,613 - __main__ - INFO - Reference: When was the Higgs boson discovered?\n",
      "2025-04-25 04:23:42,614 - __main__ - INFO - Prediction: What is the main purpose of quantum mechanics?\n",
      "2025-04-25 04:23:42,614 - __main__ - INFO - ----------------------------------------\n",
      "2025-04-25 04:23:42,615 - __main__ - INFO - Difficulty: hard\n",
      "2025-04-25 04:23:42,615 - __main__ - INFO - Reference: What is the sole permissible format for the response?\n",
      "2025-04-25 04:23:42,616 - __main__ - INFO - Prediction: What is the purpose of this paragraph?\n",
      "2025-04-25 04:23:42,616 - __main__ - INFO - ----------------------------------------\n",
      "2025-04-25 04:23:42,617 - __main__ - INFO - Difficulty: hard\n",
      "2025-04-25 04:23:42,617 - __main__ - INFO - Reference: Why would the identification of dark matter be considered a major breakthrough?\n",
      "2025-04-25 04:23:42,618 - __main__ - INFO - Prediction: What is the main goal of quantum mechanics?\n",
      "2025-04-25 04:23:42,618 - __main__ - INFO - ----------------------------------------\n",
      "2025-04-25 04:23:42,619 - __main__ - INFO - Difficulty: medium\n",
      "2025-04-25 04:23:42,619 - __main__ - INFO - Reference: What should the response consist of?\n",
      "2025-04-25 04:23:42,620 - __main__ - INFO - Prediction: What is the purpose of this paragraph?\n",
      "2025-04-25 04:23:42,620 - __main__ - INFO - ----------------------------------------\n",
      "2025-04-25 04:23:42,627 - __main__ - INFO - Saved evaluation results to results/\n",
      "2025-04-25 04:23:42,628 - __main__ - INFO - \n",
      "Demonstrating question generation:\n",
      "2025-04-25 04:23:42,628 - __main__ - INFO - \n",
      "EASY questions:\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:44,295 - __main__ - INFO - 1. What are some of the main challenges facing climate change?\n",
      "2025-04-25 04:23:44,295 - __main__ - INFO - 2. What are some of the main causes of climate change?\n",
      "2025-04-25 04:23:44,296 - __main__ - INFO - 3. What are some of the challenges facing climate change?\n",
      "2025-04-25 04:23:44,296 - __main__ - INFO - \n",
      "MEDIUM questions:\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:45,893 - __main__ - INFO - 1. What are some of the main challenges facing climate change?\n",
      "2025-04-25 04:23:45,894 - __main__ - INFO - 2. What are some of the challenges facing climate change?\n",
      "2025-04-25 04:23:45,894 - __main__ - INFO - 3. What is the role of carbon dioxide in climate change?\n",
      "2025-04-25 04:23:45,895 - __main__ - INFO - \n",
      "HARD questions:\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2025-04-25 04:23:47,451 - __main__ - INFO - 1. What are some of the main causes of climate change?\n",
      "2025-04-25 04:23:47,452 - __main__ - INFO - 2. What is the impact of climate change on marine life?\n",
      "2025-04-25 04:23:47,452 - __main__ - INFO - 3. What is the role of carbon dioxide in climate change?\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the full question generation pipeline:\n",
    "    1. Set up model and data\n",
    "    2. Train the model\n",
    "    3. Evaluate the model using both BLEU and ROUGE scores\n",
    "    4. Demonstrate question generation\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    data_path = \"Json_merged_with_difficulty.json\"  \n",
    "    batch_size = 4  \n",
    "    learning_rate = 2e-5  \n",
    "    num_epochs = 1  \n",
    "    \n",
    "    model, bert_tokenizer, gpt2_tokenizer = create_bert_gpt2_model()\n",
    "    \n",
    "    data = load_data(data_path)\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "        data, bert_tokenizer, gpt2_tokenizer, \n",
    "        train_size=0.8, val_size=0.1, test_size=0.1\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),  \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model = train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        device, \n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    best_model_path = os.path.join(\"bert_gpt2_qg_model\", \"best_model.pt\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        logger.info(f\"Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    logger.info(\"\\n--- BLEU Score Evaluation ---\")\n",
    "    bleu_score, bleu_predictions, bleu_references = evaluate_model(\n",
    "        model, \n",
    "        test_dataloader, \n",
    "        gpt2_tokenizer, \n",
    "        device,\n",
    "    )\n",
    "    \n",
    "    logger.info(\"\\n--- ROUGE Score Evaluation ---\")\n",
    "    small_test_dataset = QuestionGenerationDataset(data[-10:], bert_tokenizer, gpt2_tokenizer)\n",
    "    small_test_dataloader = DataLoader(small_test_dataset, batch_size=2)\n",
    "    \n",
    "    rouge_scores, rouge_predictions, rouge_references = evaluate_with_rouge(\n",
    "        model,\n",
    "        small_test_dataloader,\n",
    "        gpt2_tokenizer,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    output_dir = \"results\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"predictions.txt\"), \"w\") as f:\n",
    "        for prediction in bleu_predictions:\n",
    "            f.write(prediction + \"\\n\")\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"references.txt\"), \"w\") as f:\n",
    "        for reference in bleu_references:\n",
    "            f.write(reference + \"\\n\")\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"rouge_scores.json\"), \"w\") as f:\n",
    "        json.dump(rouge_scores, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Saved evaluation results to {output_dir}/\")\n",
    "    \n",
    "    sample_context = \"\"\"\n",
    "    Climate change refers to long-term shifts in temperatures and weather patterns. \n",
    "    These shifts may be natural, but since the 1800s, human activities have been the \n",
    "    main driver of climate change, primarily due to the burning of fossil fuels \n",
    "    (like coal, oil, and gas), which produces heat-trapping gases. The consequences \n",
    "    of climate change include more frequent and intense droughts, storms, heat waves, \n",
    "    rising sea levels, melting glaciers, and warming oceans which can directly harm \n",
    "    animals, destroy the places they live, and disrupt people's livelihoods.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"\\nDemonstrating question generation:\")\n",
    "    \n",
    "    for difficulty in ['easy', 'medium', 'hard']:\n",
    "        logger.info(f\"\\n{difficulty.upper()} questions:\")\n",
    "        questions = generate_questions(\n",
    "            model, \n",
    "            bert_tokenizer, \n",
    "            gpt2_tokenizer, \n",
    "            sample_context, \n",
    "            difficulty=difficulty, \n",
    "            num_questions=3, \n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            logger.info(f\"{i}. {question}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
